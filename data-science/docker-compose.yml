
services:
  # HDFS Services
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - "9870:9870"
      - "9000:9000"
    networks:
      - big-data-network

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode1
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    networks:
      - big-data-network

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode2
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    networks:
      - big-data-network

  # YARN Resource Management
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop2.7.4-java8
    container_name: resourcemanager
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864"
    env_file:
      - ./hadoop.env
    ports:
      - "8088:8088"
    networks:
      - big-data-network

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop2.7.4-java8
    container_name: nodemanager
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    networks:
      - big-data-network

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop2.7.4-java8
    container_name: historyserver
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    ports:
      - "8188:8188"
    networks:
      - big-data-network

  # PostgreSQL para metastore y otras bases de datos
  postgres_db:
    image: postgres:13
    container_name: postgres_db
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_USER: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init/postgres-init:/docker-entrypoint-initdb.d
    ports:
      - "${PORT_DB}:5432"
    networks:
      - big-data-network

  # PGAdmin para gestión de PostgreSQL
  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    ports:
      - "${PORT_PGA}:80"
    depends_on:
      - postgres_db
    networks:
      - big-data-network

  # Hive Metastore PostgreSQL
  hive-metastore-postgresql:
    image: postgres:13
    container_name: hive-metastore-postgresql
    volumes:
      - hive_postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: ${HIVE_USER}
      POSTGRES_PASSWORD: ${HIVE_PASSWORD}
    networks:
      - big-data-network

  # Hive Metastore
  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 hive-metastore-postgresql:5432"
    env_file:
      - ./hadoop.env
    volumes:
      - ./conf/hive-conf/hive-site.xml:/opt/hive/conf/hive-site.xml
    command: /opt/hive/bin/hive --service metastore
    networks:
      - big-data-network

  # Hive Server
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    environment:
      SERVICE_PRECONDITION: "hive-metastore:9083"
    env_file:
      - ./hadoop.env
    volumes:
      - ./conf/hive-conf/hive-site.xml:/opt/hive/conf/hive-site.xml
    ports:
      - "10000:10000"
      - "10002:10002"
    command: /opt/hive/bin/hive --service hiveserver2
    networks:
      - big-data-network

  # Spark Master
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    volumes:
      - ./conf/spark-conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    networks:
      - big-data-network

  # Spark Worker 1
  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_CORES=2"
      - "SPARK_WORKER_MEMORY=2G"
    volumes:
      - ./conf/spark-conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    networks:
      - big-data-network

  # Spark Worker 2
  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_CORES=2"
      - "SPARK_WORKER_MEMORY=2G"
    volumes:
      - ./conf/spark-conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    networks:
      - big-data-network

  # Spark History Server
  spark-history:
    image: bde2020/spark-history-server:3.1.1-hadoop3.2
    container_name: spark-history
    depends_on:
      - spark-master
    ports:
      - "18080:18080"
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=hdfs://namenode:9000/spark-logs
    volumes:
      - ./conf/spark-conf/spark-defaults.conf:/spark/conf/spark-defaults.conf
    networks:
      - big-data-network

  # Jupyter Notebook para análisis de datos
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      - SPARK_OPTS="--master=spark://spark-master:7077 --driver-memory=1G --executor-memory=1G"
    networks:
      - big-data-network

  # Apache NiFi para la ingesta de datos
  nifi:
    image: apache/nifi:latest
    container_name: nifi
    ports:
      - "8090:8080"
    environment:
      - NIFI_WEB_HTTP_PORT=8080
    volumes:
      - nifi_data:/opt/nifi/nifi-current/data
      - ./conf/nifi-conf:/opt/nifi/nifi-current/conf
    networks:
      - big-data-network

  # Apache Airflow para orquestación de tareas
  airflow-webserver:
    image: apache/airflow:2.5.1
    container_name: airflow-webserver
    depends_on:
      - postgres_db
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_USER}:${AIRFLOW_PASSWORD}@postgres_db:5432/${AIRFLOW_DB}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: webserver
    ports:
      - "8089:8080"
    networks:
      - big-data-network

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.5.1
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_USER}:${AIRFLOW_PASSWORD}@postgres_db:5432/${AIRFLOW_DB}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: scheduler
    networks:
      - big-data-network

  # Apache Superset para visualización de datos
  superset:
    image: apache/superset:latest
    container_name: superset
    depends_on:
      - postgres_db
    environment:
      - SUPERSET_SECRET_KEY=${SUPERSET_SECRET_KEY}
    ports:
      - "8088:8088"
    networks:
      - big-data-network

  # Redis para caché y colas
  redis:
    image: redis:6.2
    container_name: redis
    command: redis-server --requirepass ${REDIS_PASSWORD}
    ports:
      - "6379:6379"
    networks:
      - big-data-network

  # HUE - Hadoop User Experience
  hue:
    image: gethue/hue:latest
    container_name: hue
    ports:
      - "8888:8888"
    volumes:
      - ./conf/hue/z-hue.ini:/usr/share/hue/desktop/conf/z-hue.ini
    depends_on:
      - postgres_db
      - namenode
      - hive-server
    networks:
      - big-data-network

  # MongoDB para datos no estructurados
  mongodb:
    image: mongo:5.0
    container_name: mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - big-data-network

networks:
  big-data-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_historyserver:
  postgres_data:
  pgadmin_data:
  hive_postgres_data:
  nifi_data:
  mongodb_data: